{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Home/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import helpers as analysis\n",
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       $48.71/340 grams\n",
       "1       $30.78/340 grams\n",
       "2       $16.00/340 grams\n",
       "3       $19.00/340 grams\n",
       "4       $16.50/340 grams\n",
       "              ...       \n",
       "2272    $16.00/340 grams\n",
       "2273    $10.00/340 grams\n",
       "2274    $10.00/340 grams\n",
       "2275    $16.00/340 grams\n",
       "2276    $20.00/340 grams\n",
       "Name: est_price, Length: 2277, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/data_cleaning_coffee.csv')\n",
    "df['est_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_descriptions = df[[\"desc_1\"]].apply(lambda x : ' '.join(x.dropna()), axis=1)\n",
    "combined_names = df[[\"name\"]].apply(lambda x : ' '.join(x.dropna()), axis=1)\n",
    "combined_locs = df[[\"origin\"]].apply(lambda x : ' '.join(x.dropna()), axis=1)\n",
    "combined_prices = df[[\"est_price\"]].apply(lambda x : ' '.join(x.dropna()), axis=1)\n",
    "\n",
    "combined_descriptions = [x for x in combined_descriptions]\n",
    "combined_names = [x + \" from \" + combined_locs[i] for i, x in enumerate(combined_names)]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "#Replace query with the user query here \n",
    "query = [\"Nutty and Sweet\"]\n",
    "# query = query.lower()\n",
    "# query = query.split(\" \")\n",
    "\n",
    "doc_vectors  = vectorizer.fit_transform(query + combined_descriptions).toarray()\n",
    "\n",
    "index_to_vocab = {i:v for i, v in enumerate(vectorizer.get_feature_names_out())}\n",
    "doc_to_index = {v:i for i, v in enumerate(combined_names)}\n",
    "index_to_doc_descriptions = {i:{'name': v, 'description': combined_descriptions[i]}for i, v in enumerate(combined_names)}\n",
    "\n",
    "cosineSims = (np.dot(doc_vectors[0], np.transpose(doc_vectors[1:]))/(LA.norm(doc_vectors[0])* LA.norm(doc_vectors[1:])))\n",
    "\n",
    "cosineSims = [(x, i) for i, x in enumerate(cosineSims)]\n",
    "\n",
    "cosineSims = sorted(cosineSims, key = lambda x:x[0], reverse = True)\n",
    "\n",
    "topTen = cosineSims[:10]\n",
    "answer  = []\n",
    "for sim, index in topTen: \n",
    "  answer.append((index_to_doc_descriptions[index], sim))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ']' does not match opening parenthesis '(' (1687839994.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    price = [float(x[re.find([$]+1:x.find('/')]) for x in price]\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis ']' does not match opening parenthesis '('\n"
     ]
    }
   ],
   "source": [
    "price = [x for x in combined_prices]\n",
    "for i, x in enumerate(price):\n",
    "  price = [float(x[re.find([$]+1:x.find('/'))]) for x in price]\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31395688 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "search_terms = \"my query\"\n",
    "documents = [\"my\",\"list\",\"of\",\"docs\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "doc_vectors = vectorizer.fit_transform([search_terms] + documents).toarray()\n",
    "\n",
    "print(np.dot(doc_vectors[0], np.transpose(doc_vectors[1:]))/(LA.norm(doc_vectors[0])* LA.norm(doc_vectors[1:])))\n",
    "\n",
    "# document_scores = [item.item() for item in cosine_similarities[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTopTen(user_query):\n",
    "    \"\"\"\n",
    "    Takes a user query and returns an array of (dictionary, integer) pairs\n",
    "    Dictionary is a dictionary of 'name', and 'description' of a coffee, and the integer is the cosine similarity\n",
    "\n",
    "    Result is returned in order of cosine similarity highest to lowest\n",
    "    User query must be input as a string\n",
    "    \"\"\"\n",
    "\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from collections import Counter\n",
    "    import string\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    import helpers as analysis\n",
    "    from numpy import linalg as LA\n",
    "    import numpy as np\n",
    "\n",
    "    df = pd.read_csv(\"data/data_cleaning_coffee.csv\")\n",
    "\n",
    "    combined_descriptions = df[[\"desc_1\"]].apply(lambda x: \" \".join(x.dropna()), axis=1)\n",
    "    combined_names = df[[\"name\"]].apply(lambda x: \" \".join(x.dropna()), axis=1)\n",
    "    combined_locs = df[[\"origin\"]].apply(lambda x: \" \".join(x.dropna()), axis=1)\n",
    "\n",
    "    combined_descriptions = [x for x in combined_descriptions]\n",
    "    combined_names = [\n",
    "        x + \" from \" + combined_locs[i] for i, x in enumerate(combined_names)\n",
    "    ]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Replace query with the user query here\n",
    "    query = [user_query]\n",
    "\n",
    "    doc_vectors = vectorizer.fit_transform(query + combined_descriptions).toarray()\n",
    "    # don't think we need these, but in the event that we have a very slow query, we can use this\n",
    "    # index_to_vocab = {i: v for i, v in enumerate(vectorizer.get_feature_names_out())}\n",
    "    # doc_to_index = {v: i for i, v in enumerate(combined_names)}\n",
    "    index_to_doc_descriptions = {\n",
    "        i: {\"name\": v, \"description\": combined_descriptions[i]}\n",
    "        for i, v in enumerate(combined_names)\n",
    "    }\n",
    "\n",
    "    cosineSims = np.dot(doc_vectors[0], np.transpose(doc_vectors[1:])) / (\n",
    "        LA.norm(doc_vectors[0]) * LA.norm(doc_vectors[1:])\n",
    "    )\n",
    "\n",
    "    cosineSims = [(x, i) for i, x in enumerate(cosineSims)]\n",
    "\n",
    "    cosineSims = sorted(cosineSims, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    topTen = cosineSims[:10]\n",
    "    answer = []\n",
    "    for sim, index in topTen:\n",
    "        answer.append((index_to_doc_descriptions[index], sim))\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finca Lerida Gesha Natural from Jaramillo, Boquete growing region, western Panama\n",
      "Richly sweet, floral-toned. Magnolia, date, almond, caramel, fresh-cut cedar in aroma and cup. Sweet structure with gentle, round acidity; full, syrupy-smooth mouthfeel. Floral and nut-toned finish.\n",
      "0.009647394581102009\n",
      "Costa Rica Central Valley Brandy Ball from Central Valley, Costa Rica\n",
      "Richly sweet, floral-toned. Freesia-like flowers, orange zest, hazelnut, caramel, cedar in aroma and cup. Sweet structure with gentle, round acidity; full, syrupy-smooth mouthfeel. Crisp floral- and nut-toned finish.\n",
      "0.009341387794646074\n",
      "Panama Geisha from Jaramillo, Boquete growing region, Panama\n",
      "Richly sweet, floral-toned. Honeysuckle, raisin, pistachio, caramel, fresh-cut cedar in aroma and cup. Sweet-toned structure with gentle, round acidity; velvety-smooth mouthfeel. Floral and nut-toned finish.\n",
      "0.008860441548207946\n",
      "Honduras Relationship Coffee Norma Lara from Ocotepeque Department, Honduras\n",
      "Richly floral-toned, deeply sweet. Jasmine, grapefruit zest, cocoa nib, marjoram, maple syrup in aroma and cup. Sweet-toned structure with juicy, bright acidity; syrupy-smooth mouthfeel. Balanced, floral, citrusy finish.\n",
      "0.008711737949949313\n",
      "Organic Panorama from Yirgacheffe growing region, southern Ethiopia\n",
      "Crisply sweet, floral-toned. Narcissus, plum, almond, baking chocolate, a hint of bay leaf in aroma and cup. Sweet structure with round, gentle acidity; full, syrupy mouthfeel. Chocolaty and floral finish, crisp and resonant.\n",
      "0.008639057570572076\n",
      "Ethiopia Nano Genji #5 from Guji Zone, Oromia Region, southern Ethiopia\n",
      "Richly floral-toned, deeply sweet. Narcissus, Meyer lemon zest, cocoa nib, marjoram, maple syrup in aroma and cup. Sweet-toned structure with juicy, bright acidity; syrupy-smooth mouthfeel. Balanced floral, citrusy finish.\n",
      "0.008273698437151079\n",
      "Guatemala Coffea Diversa Geisha Queen Honey from Suchitan, Guatemala\n",
      "Delicately sweet, floral-toned. Honeysuckle, lemon verbena, pink peppercorn, baking chocolate, hazelnut in aroma and cup. Sweet structure with vibrant, balanced acidity; satiny mouthfeel. Floral-toned finish with hints of baking chocolate.\n",
      "0.007831645062649696\n",
      "Ethiopia Nano Genji #4 from Agaro Gera, Jimma Zone, Oromia State, Ethiopia\n",
      "Richly sweet, floral-toned. Candied violet, almond nougat, maple syrup, date, cocoa nib in aroma and cup. Sweet-toned structure with gentle, round acidity; viscous, satiny mouthfeel. Resonant floral- and cocoa-laden finish.\n",
      "0.007817768596924563\n",
      "Finca Santa Isabel from Cob√°n, Alta Verapaz Department, Guatemala\n",
      "Sweet-toned, floral. Magnolia, yellow plum, almond, cane sugar, cedar in aroma and cup. Sweetly tart structure with gently bright acidity; satiny mouthfeel. Floral- and aromatic wood-toned finish.\n",
      "0.007664822603038263\n",
      "Ethiopia Grade 1 Chelchele from Yirgacheffe growing region, southern Ethiopia\n",
      "Delicate, richly aromatic, floral-toned. Honeysuckle, cocoa nib, pink grapefruit zest, apricot, marjoram in aroma and cup. Sweetly tart with juicy, high-toned acidity; plush, syrupy mouthfeel. Floral- and cocoa-driven finish.\n",
      "0.007177347614244417\n"
     ]
    }
   ],
   "source": [
    "answer = findTopTen(\"sweet and floral\")\n",
    "\n",
    "for i, x in enumerate(answer): \n",
    "  print(x[0]['name'])\n",
    "  print(x[0]['description'])\n",
    "  print(x[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
