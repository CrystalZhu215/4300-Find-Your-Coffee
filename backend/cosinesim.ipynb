{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import helpers as analysis\n",
    "from numpy import linalg as LA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_cleaning_coffee.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_descriptions = df[[\"desc_1\"]].apply(lambda x : ' '.join(x.dropna()), axis=1)\n",
    "combined_names = df[[\"name\"]].apply(lambda x : ' '.join(x.dropna()), axis=1)\n",
    "combined_locs = df[[\"origin\"]].apply(lambda x : ' '.join(x.dropna()), axis=1)\n",
    "\n",
    "combined_descriptions = [x for x in combined_descriptions]\n",
    "combined_names = [x + \" from \" + combined_locs[i] for i, x in enumerate(combined_names)]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "#Replace query with the user query here \n",
    "query = [\"Nutty and Sweet\"]\n",
    "# query = query.lower()\n",
    "# query = query.split(\" \")\n",
    "\n",
    "doc_vectors  = vectorizer.fit_transform(query + combined_descriptions).toarray()\n",
    "\n",
    "index_to_vocab = {i:v for i, v in enumerate(vectorizer.get_feature_names_out())}\n",
    "doc_to_index = {v:i for i, v in enumerate(combined_names)}\n",
    "index_to_doc_descriptions = {i:{'name': v, 'description': combined_descriptions[i]}for i, v in enumerate(combined_names)}\n",
    "\n",
    "cosineSims = (np.dot(doc_vectors[0], np.transpose(doc_vectors[1:]))/(LA.norm(doc_vectors[0])* LA.norm(doc_vectors[1:])))\n",
    "\n",
    "cosineSims = [(x, i) for i, x in enumerate(cosineSims)]\n",
    "\n",
    "cosineSims = sorted(cosineSims, key = lambda x:x[0], reverse = True)\n",
    "\n",
    "topTen = cosineSims[:10]\n",
    "answer  = []\n",
    "for sim, index in topTen: \n",
    "  answer.append((index_to_doc_descriptions[index], sim))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = \"my query\"\n",
    "documents = [\"my\",\"list\",\"of\",\"docs\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "doc_vectors = vectorizer.fit_transform([search_terms] + documents).toarray()\n",
    "\n",
    "print(np.dot(doc_vectors[0], np.transpose(doc_vectors[1:]))/(LA.norm(doc_vectors[0])* LA.norm(doc_vectors[1:])))\n",
    "\n",
    "# document_scores = [item.item() for item in cosine_similarities[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTopTen(user_query):\n",
    "    \"\"\"\n",
    "    Takes a user query and returns an array of (dictionary, integer) pairs\n",
    "    Dictionary is a dictionary of 'name', and 'description' of a coffee, and the integer is the cosine similarity\n",
    "\n",
    "    Result is returned in order of cosine similarity highest to lowest\n",
    "    User query must be input as a string\n",
    "    \"\"\"\n",
    "\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from collections import Counter\n",
    "    import string\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    import helpers as analysis\n",
    "    from numpy import linalg as LA\n",
    "    import numpy as np\n",
    "\n",
    "    df = pd.read_csv(\"data/data_cleaning_coffee.csv\")\n",
    "\n",
    "    combined_descriptions = df[[\"desc_1\"]].apply(lambda x: \" \".join(x.dropna()), axis=1)\n",
    "    combined_names = df[[\"name\"]].apply(lambda x: \" \".join(x.dropna()), axis=1)\n",
    "    combined_locs = df[[\"origin\"]].apply(lambda x: \" \".join(x.dropna()), axis=1)\n",
    "\n",
    "    combined_descriptions = [x for x in combined_descriptions]\n",
    "    combined_names = [\n",
    "        x + \" from \" + combined_locs[i] for i, x in enumerate(combined_names)\n",
    "    ]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Replace query with the user query here\n",
    "    query = [user_query]\n",
    "\n",
    "    doc_vectors = vectorizer.fit_transform(query + combined_descriptions).toarray()\n",
    "    # don't think we need these, but in the event that we have a very slow query, we can use this\n",
    "    # index_to_vocab = {i: v for i, v in enumerate(vectorizer.get_feature_names_out())}\n",
    "    # doc_to_index = {v: i for i, v in enumerate(combined_names)}\n",
    "    index_to_doc_descriptions = {\n",
    "        i: {\"name\": v, \"description\": combined_descriptions[i]}\n",
    "        for i, v in enumerate(combined_names)\n",
    "    }\n",
    "\n",
    "    cosineSims = np.dot(doc_vectors[0], np.transpose(doc_vectors[1:])) / (\n",
    "        LA.norm(doc_vectors[0]) * LA.norm(doc_vectors[1:])\n",
    "    )\n",
    "\n",
    "    cosineSims = [(x, i) for i, x in enumerate(cosineSims)]\n",
    "\n",
    "    cosineSims = sorted(cosineSims, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    topTen = cosineSims[:10]\n",
    "    answer = []\n",
    "    for sim, index in topTen:\n",
    "        answer.append((index_to_doc_descriptions[index], sim))\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = findTopTen(\"sweet and floral\")\n",
    "\n",
    "for i, x in enumerate(answer): \n",
    "  print(x[0]['name'])\n",
    "  print(x[0]['description'])\n",
    "  print(x[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
